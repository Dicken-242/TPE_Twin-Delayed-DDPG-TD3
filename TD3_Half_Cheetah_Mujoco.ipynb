{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# TPE_Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installation des paquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "executionInfo": {
     "elapsed": 19414,
     "status": "ok",
     "timestamp": 1601843113950,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "HAHMB0Ze8fU0",
    "outputId": "73d07cd5-8b18-4a5b-e25d-4562aa7300e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybullet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/52/6ae1b6d2e38de3a0dddbdc7297fea0f2c41def3ceb53390fa4a305fe0efe/pybullet-3.0.4-cp36-cp36m-manylinux1_x86_64.whl (102.2MB)\n",
      "\u001b[K     |████████████████████████████████| 102.2MB 64kB/s \n",
      "\u001b[?25hInstalling collected packages: pybullet\n",
      "Successfully installed pybullet-3.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dHrHyHwyq1S"
   },
   "source": [
    "#Importation des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4138,
     "status": "ok",
     "timestamp": 1601843129308,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Etape 1: On itinialise Experience Replay memory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1601843131522,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Etape 2: Nous construisons un réseau neuronal pour le modèle Actor et un réseau neuronal pour la cible Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1601843133358,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Étape 3 : Nous construisons deux réseaux de neurones pour les deux modèles Critic et deux réseaux de neurones pour les deux cibles Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1601843135075,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Définir le premier réseau neuronal critique\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Définir le second réseau neuronal critique\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Propagation de l'information sur le premier réseau neuronal critique\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Propagation de l'information sur le second réseau neuronal critique\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Étapes 4 à 15 : Processus de d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1601843136713,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "#  Device selection (CPU ou GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Intégrer l'ensemble du processus de formation dans une classe\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Étape 4: Nous échantillonnons un lot de transitions (s, s’, a, r) de la mémoire\n",
    "\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Étape 5: De l'état suivant s', l'acteur cible joue l'action suivante a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Étape 6: Nous ajoutons le bruit gaussien à cette prochaine action a’ et nous l'inscrivons dans une série de valeurs soutenues par l'environnement\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Étape 7: Les deux cibles des critiques prennent chacune le couple (s’, a’) comme input et retourne deux Q-values Qt1(s’,a’) et Qt2(s’,a’) comme outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Étape 8: Nous gardons le minimum de ces deux Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Étape 9: Nous obtenons la cible finale des deux modèles critiques, qui est: Qt = r + γ * min(Qt1, Qt2), où γ est le coefficient d'actualisation\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Étape 10: Les deux modèles critiques prennent chacun le couple (s, a) comme input et retourne deux Q-values Q1(s,a) et Q2(s,a) comme outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Étape 11: Nous calculons la perte à partir des deux modèles critiques: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Étape 12: Nous rétropropageons cette perte Critic et mettons à jour les paramètres des deux modèles Critic avec un optimiseur SGD\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Étape 13:  Une fois pour chaque deux itérations, nous mettons à jour notre modèle Acteur en effectuant une montée du gradient sur la sortie du premier modèle Critique\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Étape 14: Une fois pour chaque deux itérations, nous mettons à jour les poids de l'acteur cible en calculant la moyenne des polyak\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Étape 15 : Une fois pour chaque deux itérations, nous mettons à jour les poids de la cible critique en calculant la moyenne des polyak\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Faire une méthode de sauvegarde pour sauvegarder un modèle entrainé\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Mise au point d'une méthode de chargement pour charger un modèle pré-entrainé\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## Nous faisons une fonction qui évalue la politique en calculant sa récompense moyenne sur 10 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1601843138111,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  \n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Ce texte est au format code\n",
    "```\n",
    "\n",
    "## Nous fixons les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1601843139545,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\" # Nom de l'environnement (Nous le reglons sur environnement continu que nous souhaitons)\n",
    "seed = 0 # Numéro de fixation(seed) aléatoire\n",
    "start_timesteps = 1e4 # Nombre d'itérations / étapes avant lesquelles le modèle choisit une action au hasard, et après lesquelles il commence à utiliser le réseau politique\n",
    "eval_freq = 5e3 # Combien de fois l'étape d'évaluation est effectuée (après combien de temps)\n",
    "max_timesteps = 5e5 # Nombre total d'itérations / étapes\n",
    "save_models = True # Vérificateur booléen pour savoir s'il faut ou non sauver le modèle préformé\n",
    "expl_noise = 0.1 # Bruit d'exploration - Valeur STD de l'exploration Bruit gaussien\n",
    "batch_size = 100 # Taille du lot\n",
    "discount = 0.99 # Facteur d'escompte gamma, utilisé dans le calcul de la récompense totale escomptée\n",
    "tau = 0.005 # Taux de mise à jour du réseau cible\n",
    "policy_noise = 0.2 # MST du bruit gaussien ajoutée aux actions à des fins d'exploration\n",
    "noise_clip = 0.5 # Valeur maximale du bruit gaussien ajouté aux actions (politique)\n",
    "policy_freq = 2 # Nombre d'itérations à attendre avant de mettre à jour le réseau politique (modèle Actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## Nous créons un nom de fichier pour les deux modèles sauvegardés : les modèles Acteur et Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1601843141525,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "1fyH8N5z-o3o",
    "outputId": "5d1d3c61-bbdf-42c4-9ef0-e5e628a6793b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## Nous créons un dossier à l'intérieur duquel seront sauvegardés les modèles formés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1601843143062,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## Nous créons l'environnement PyBullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1601843144500,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "CyQXJUIs-6BV",
    "outputId": "fec3ddaa-9e78-4998-dfad-2cacdc6836bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## Nous mettons en place les valeurs et nous obtenons les informations nécessaires sur les états et les actions dans l'environnement choisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1601843146027,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## Nous créons le réseau politique (le modèle Actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1601843147465,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## Nous créons la mémoire de l'expérience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1601843148795,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## Nous définissons une liste où sont stockés tous les résultats d'évaluation sur 10 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "executionInfo": {
     "elapsed": 18637,
     "status": "ok",
     "timestamp": 1601843168286,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "dhC_5XJ__Orp",
    "outputId": "4d15edfb-1854-499f-fc5c-d4ae99a5c00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1429.426163\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## Nous créons un nouveau répertoire de dossiers dans lequel les résultats finaux (vidéos de l'agent) seront remplis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1601843170916,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## Nous initialisons les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1601843173284,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1722807,
     "status": "ok",
     "timestamp": 1601858803047,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "y_ouY4NH_Y0I",
    "outputId": "f9207e42-c9b1-4533-b7c1-5c8134b4ba3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: -1428.9683875246344\n",
      "Total Timesteps: 2000 Episode Num: 2 Reward: -1346.3968009963335\n",
      "Total Timesteps: 3000 Episode Num: 3 Reward: -1454.0231639096664\n",
      "Total Timesteps: 4000 Episode Num: 4 Reward: -1366.327901297108\n",
      "Total Timesteps: 5000 Episode Num: 5 Reward: -1167.8386884896897\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1495.642147\n",
      "---------------------------------------\n",
      "Total Timesteps: 6000 Episode Num: 6 Reward: -1262.1999878148674\n",
      "Total Timesteps: 7000 Episode Num: 7 Reward: -1137.836346134204\n",
      "Total Timesteps: 8000 Episode Num: 8 Reward: -1282.186879642088\n",
      "Total Timesteps: 9000 Episode Num: 9 Reward: -1268.0734087566348\n",
      "Total Timesteps: 10000 Episode Num: 10 Reward: -1414.2107823890558\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1796.337328\n",
      "---------------------------------------\n",
      "Total Timesteps: 11000 Episode Num: 11 Reward: -1733.5919930837288\n",
      "Total Timesteps: 12000 Episode Num: 12 Reward: -1670.259352583983\n",
      "Total Timesteps: 13000 Episode Num: 13 Reward: -1668.2192188710487\n",
      "Total Timesteps: 14000 Episode Num: 14 Reward: -1610.8885021274307\n",
      "Total Timesteps: 15000 Episode Num: 15 Reward: -1008.2616499247122\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1428.649677\n",
      "---------------------------------------\n",
      "Total Timesteps: 16000 Episode Num: 16 Reward: -1543.3798432763112\n",
      "Total Timesteps: 17000 Episode Num: 17 Reward: -1297.7358051459871\n",
      "Total Timesteps: 18000 Episode Num: 18 Reward: -998.701264526047\n",
      "Total Timesteps: 19000 Episode Num: 19 Reward: -1590.0986866172368\n",
      "Total Timesteps: 20000 Episode Num: 20 Reward: -934.0948025309732\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -694.128252\n",
      "---------------------------------------\n",
      "Total Timesteps: 21000 Episode Num: 21 Reward: -652.8187574430167\n",
      "Total Timesteps: 22000 Episode Num: 22 Reward: -1565.2902946892577\n",
      "Total Timesteps: 23000 Episode Num: 23 Reward: -1411.077503799236\n",
      "Total Timesteps: 24000 Episode Num: 24 Reward: -1059.6517557159839\n",
      "Total Timesteps: 25000 Episode Num: 25 Reward: -1045.6250057080188\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1154.801239\n",
      "---------------------------------------\n",
      "Total Timesteps: 26000 Episode Num: 26 Reward: -1122.9389617428915\n",
      "Total Timesteps: 27000 Episode Num: 27 Reward: -1275.940643344623\n",
      "Total Timesteps: 28000 Episode Num: 28 Reward: -812.4426719989397\n",
      "Total Timesteps: 29000 Episode Num: 29 Reward: -766.9923207957289\n",
      "Total Timesteps: 30000 Episode Num: 30 Reward: 598.9803346435347\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1558.138094\n",
      "---------------------------------------\n",
      "Total Timesteps: 31000 Episode Num: 31 Reward: -1600.868101689603\n",
      "Total Timesteps: 32000 Episode Num: 32 Reward: -1066.3642893097747\n",
      "Total Timesteps: 33000 Episode Num: 33 Reward: 364.73422700588185\n",
      "Total Timesteps: 34000 Episode Num: 34 Reward: -368.7249051315544\n",
      "Total Timesteps: 35000 Episode Num: 35 Reward: -438.09424796891835\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1342.217589\n",
      "---------------------------------------\n",
      "Total Timesteps: 36000 Episode Num: 36 Reward: -1312.3486832783506\n",
      "Total Timesteps: 37000 Episode Num: 37 Reward: -1046.0071614833405\n",
      "Total Timesteps: 38000 Episode Num: 38 Reward: -1151.2342771444994\n",
      "Total Timesteps: 39000 Episode Num: 39 Reward: -851.2101040878142\n",
      "Total Timesteps: 40000 Episode Num: 40 Reward: -994.7405240119631\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -60.552445\n",
      "---------------------------------------\n",
      "Total Timesteps: 41000 Episode Num: 41 Reward: 472.5802374783897\n",
      "Total Timesteps: 42000 Episode Num: 42 Reward: -394.7571640159035\n",
      "Total Timesteps: 43000 Episode Num: 43 Reward: 485.38990017370907\n",
      "Total Timesteps: 44000 Episode Num: 44 Reward: 547.7648707841734\n",
      "Total Timesteps: 45000 Episode Num: 45 Reward: 552.7988080516169\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -949.569767\n",
      "---------------------------------------\n",
      "Total Timesteps: 46000 Episode Num: 46 Reward: -1023.8015779486906\n",
      "Total Timesteps: 47000 Episode Num: 47 Reward: -583.7358015518236\n",
      "Total Timesteps: 48000 Episode Num: 48 Reward: 564.0825821999354\n",
      "Total Timesteps: 49000 Episode Num: 49 Reward: 17.32336016387527\n",
      "Total Timesteps: 50000 Episode Num: 50 Reward: -62.976578354552196\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 392.999599\n",
      "---------------------------------------\n",
      "Total Timesteps: 51000 Episode Num: 51 Reward: 213.02636642683373\n",
      "Total Timesteps: 52000 Episode Num: 52 Reward: 397.17412278365106\n",
      "Total Timesteps: 53000 Episode Num: 53 Reward: 453.4970256448322\n",
      "Total Timesteps: 54000 Episode Num: 54 Reward: 484.5736173086488\n",
      "Total Timesteps: 55000 Episode Num: 55 Reward: 288.9374429268853\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 423.860023\n",
      "---------------------------------------\n",
      "Total Timesteps: 56000 Episode Num: 56 Reward: 563.2919930270009\n",
      "Total Timesteps: 57000 Episode Num: 57 Reward: 181.11468798271846\n",
      "Total Timesteps: 58000 Episode Num: 58 Reward: 127.5577538173065\n",
      "Total Timesteps: 59000 Episode Num: 59 Reward: 322.9858888903046\n",
      "Total Timesteps: 60000 Episode Num: 60 Reward: -733.0949178485697\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 508.364280\n",
      "---------------------------------------\n",
      "Total Timesteps: 61000 Episode Num: 61 Reward: 366.75859025597674\n",
      "Total Timesteps: 62000 Episode Num: 62 Reward: 571.47963321015\n",
      "Total Timesteps: 63000 Episode Num: 63 Reward: 423.2395592904527\n",
      "Total Timesteps: 64000 Episode Num: 64 Reward: 215.25368953562264\n",
      "Total Timesteps: 65000 Episode Num: 65 Reward: 455.39753846594266\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 580.592943\n",
      "---------------------------------------\n",
      "Total Timesteps: 66000 Episode Num: 66 Reward: 563.3649376099995\n",
      "Total Timesteps: 67000 Episode Num: 67 Reward: 679.1964980633758\n",
      "Total Timesteps: 68000 Episode Num: 68 Reward: 448.19441391074014\n",
      "Total Timesteps: 69000 Episode Num: 69 Reward: 523.381858623637\n",
      "Total Timesteps: 70000 Episode Num: 70 Reward: 454.72483622222524\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 555.062736\n",
      "---------------------------------------\n",
      "Total Timesteps: 71000 Episode Num: 71 Reward: 358.68857386749835\n",
      "Total Timesteps: 72000 Episode Num: 72 Reward: 556.6481369849885\n",
      "Total Timesteps: 73000 Episode Num: 73 Reward: 762.4728859305775\n",
      "Total Timesteps: 74000 Episode Num: 74 Reward: 569.879608697149\n",
      "Total Timesteps: 75000 Episode Num: 75 Reward: 562.0830707017542\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 562.985424\n",
      "---------------------------------------\n",
      "Total Timesteps: 76000 Episode Num: 76 Reward: 564.1607378578474\n",
      "Total Timesteps: 77000 Episode Num: 77 Reward: 690.1359892402902\n",
      "Total Timesteps: 78000 Episode Num: 78 Reward: 687.8167575174106\n",
      "Total Timesteps: 79000 Episode Num: 79 Reward: 653.4900243488179\n",
      "Total Timesteps: 80000 Episode Num: 80 Reward: -823.8842015036256\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 591.468608\n",
      "---------------------------------------\n",
      "Total Timesteps: 81000 Episode Num: 81 Reward: 734.8104463169318\n",
      "Total Timesteps: 82000 Episode Num: 82 Reward: 690.1860589861822\n",
      "Total Timesteps: 83000 Episode Num: 83 Reward: 544.675739747919\n",
      "Total Timesteps: 84000 Episode Num: 84 Reward: 560.7249203300044\n",
      "Total Timesteps: 85000 Episode Num: 85 Reward: 699.2777704036668\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 664.287455\n",
      "---------------------------------------\n",
      "Total Timesteps: 86000 Episode Num: 86 Reward: 608.5531184438137\n",
      "Total Timesteps: 87000 Episode Num: 87 Reward: 303.88468914614924\n",
      "Total Timesteps: 88000 Episode Num: 88 Reward: 731.8386706679571\n",
      "Total Timesteps: 89000 Episode Num: 89 Reward: 671.1846666028985\n",
      "Total Timesteps: 90000 Episode Num: 90 Reward: 583.8624071774582\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 604.261827\n",
      "---------------------------------------\n",
      "Total Timesteps: 91000 Episode Num: 91 Reward: 688.6496200579816\n",
      "Total Timesteps: 92000 Episode Num: 92 Reward: 798.8361978709179\n",
      "Total Timesteps: 93000 Episode Num: 93 Reward: 666.2844022236748\n",
      "Total Timesteps: 94000 Episode Num: 94 Reward: 237.6571477121086\n",
      "Total Timesteps: 95000 Episode Num: 95 Reward: 694.5129950143404\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 749.922670\n",
      "---------------------------------------\n",
      "Total Timesteps: 96000 Episode Num: 96 Reward: 685.5693632010561\n",
      "Total Timesteps: 97000 Episode Num: 97 Reward: 684.2720309142014\n",
      "Total Timesteps: 98000 Episode Num: 98 Reward: 662.5820554894373\n",
      "Total Timesteps: 99000 Episode Num: 99 Reward: 466.77041188936516\n",
      "Total Timesteps: 100000 Episode Num: 100 Reward: 662.9963991217102\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 601.612880\n",
      "---------------------------------------\n",
      "Total Timesteps: 101000 Episode Num: 101 Reward: 590.8867814614046\n",
      "Total Timesteps: 102000 Episode Num: 102 Reward: 560.8050339259642\n",
      "Total Timesteps: 103000 Episode Num: 103 Reward: 671.2308739679437\n",
      "Total Timesteps: 104000 Episode Num: 104 Reward: 728.5522186459428\n",
      "Total Timesteps: 105000 Episode Num: 105 Reward: 818.2255208418599\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 700.522369\n",
      "---------------------------------------\n",
      "Total Timesteps: 106000 Episode Num: 106 Reward: 853.2181178454281\n",
      "Total Timesteps: 107000 Episode Num: 107 Reward: 737.2795997984209\n",
      "Total Timesteps: 108000 Episode Num: 108 Reward: 652.938375414505\n",
      "Total Timesteps: 109000 Episode Num: 109 Reward: 632.1333517903073\n",
      "Total Timesteps: 110000 Episode Num: 110 Reward: 636.8074762073577\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 630.228755\n",
      "---------------------------------------\n",
      "Total Timesteps: 111000 Episode Num: 111 Reward: 622.4874853737509\n",
      "Total Timesteps: 112000 Episode Num: 112 Reward: 687.7784091843465\n",
      "Total Timesteps: 113000 Episode Num: 113 Reward: 681.1838457480974\n",
      "Total Timesteps: 114000 Episode Num: 114 Reward: 772.4586797308033\n",
      "Total Timesteps: 115000 Episode Num: 115 Reward: 772.865647672075\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 746.511738\n",
      "---------------------------------------\n",
      "Total Timesteps: 116000 Episode Num: 116 Reward: 852.8636665739658\n",
      "Total Timesteps: 117000 Episode Num: 117 Reward: 597.6696928215479\n",
      "Total Timesteps: 118000 Episode Num: 118 Reward: 678.4923694160568\n",
      "Total Timesteps: 119000 Episode Num: 119 Reward: 573.0380693081476\n",
      "Total Timesteps: 120000 Episode Num: 120 Reward: 693.9317765784407\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 696.456259\n",
      "---------------------------------------\n",
      "Total Timesteps: 121000 Episode Num: 121 Reward: 751.8540137214927\n",
      "Total Timesteps: 122000 Episode Num: 122 Reward: 661.9000639503979\n",
      "Total Timesteps: 123000 Episode Num: 123 Reward: 884.1524516925549\n",
      "Total Timesteps: 124000 Episode Num: 124 Reward: 687.5511175417769\n",
      "Total Timesteps: 125000 Episode Num: 125 Reward: 770.3929873731737\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 818.853937\n",
      "---------------------------------------\n",
      "Total Timesteps: 126000 Episode Num: 126 Reward: 824.0107062879612\n",
      "Total Timesteps: 127000 Episode Num: 127 Reward: 683.0727698791686\n",
      "Total Timesteps: 128000 Episode Num: 128 Reward: 538.5701698404293\n",
      "Total Timesteps: 129000 Episode Num: 129 Reward: 596.8290755126585\n",
      "Total Timesteps: 130000 Episode Num: 130 Reward: 785.0899956959748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 640.596673\n",
      "---------------------------------------\n",
      "Total Timesteps: 131000 Episode Num: 131 Reward: 608.0747288347562\n",
      "Total Timesteps: 132000 Episode Num: 132 Reward: 638.6108741497322\n",
      "Total Timesteps: 133000 Episode Num: 133 Reward: 625.434975960903\n",
      "Total Timesteps: 134000 Episode Num: 134 Reward: 766.1185705778255\n",
      "Total Timesteps: 135000 Episode Num: 135 Reward: 852.3344736079662\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 911.423391\n",
      "---------------------------------------\n",
      "Total Timesteps: 136000 Episode Num: 136 Reward: 944.842961408373\n",
      "Total Timesteps: 137000 Episode Num: 137 Reward: 852.652060610139\n",
      "Total Timesteps: 138000 Episode Num: 138 Reward: 812.4746956320533\n",
      "Total Timesteps: 139000 Episode Num: 139 Reward: 951.9832031580635\n",
      "Total Timesteps: 140000 Episode Num: 140 Reward: 1154.7167328387227\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1012.970202\n",
      "---------------------------------------\n",
      "Total Timesteps: 141000 Episode Num: 141 Reward: 934.0079212159808\n",
      "Total Timesteps: 142000 Episode Num: 142 Reward: 812.057225140174\n",
      "Total Timesteps: 143000 Episode Num: 143 Reward: 805.2554602065688\n",
      "Total Timesteps: 144000 Episode Num: 144 Reward: 920.6432489068873\n",
      "Total Timesteps: 145000 Episode Num: 145 Reward: 1092.7388880009216\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 815.399706\n",
      "---------------------------------------\n",
      "Total Timesteps: 146000 Episode Num: 146 Reward: 984.4101486239138\n",
      "Total Timesteps: 147000 Episode Num: 147 Reward: 1113.0564235408244\n",
      "Total Timesteps: 148000 Episode Num: 148 Reward: 1129.4198624405335\n",
      "Total Timesteps: 149000 Episode Num: 149 Reward: 1075.860634262723\n",
      "Total Timesteps: 150000 Episode Num: 150 Reward: 1247.1302896026841\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 755.694806\n",
      "---------------------------------------\n",
      "Total Timesteps: 151000 Episode Num: 151 Reward: 749.6248744587017\n",
      "Total Timesteps: 152000 Episode Num: 152 Reward: 588.6749809422898\n",
      "Total Timesteps: 153000 Episode Num: 153 Reward: 1067.019247282038\n",
      "Total Timesteps: 154000 Episode Num: 154 Reward: 1190.0597929175422\n",
      "Total Timesteps: 155000 Episode Num: 155 Reward: 1283.1973877758005\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1236.627326\n",
      "---------------------------------------\n",
      "Total Timesteps: 156000 Episode Num: 156 Reward: 1241.7401204610496\n",
      "Total Timesteps: 157000 Episode Num: 157 Reward: 1323.9578788140486\n",
      "Total Timesteps: 158000 Episode Num: 158 Reward: 894.3103637640673\n",
      "Total Timesteps: 159000 Episode Num: 159 Reward: 1097.4050770739825\n",
      "Total Timesteps: 160000 Episode Num: 160 Reward: -1372.0896602242765\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 464.350262\n",
      "---------------------------------------\n",
      "Total Timesteps: 161000 Episode Num: 161 Reward: 1098.5497684219756\n",
      "Total Timesteps: 162000 Episode Num: 162 Reward: 1093.7480333397543\n",
      "Total Timesteps: 163000 Episode Num: 163 Reward: 1236.2726223766294\n",
      "Total Timesteps: 164000 Episode Num: 164 Reward: -665.4241737074989\n",
      "Total Timesteps: 165000 Episode Num: 165 Reward: 864.4417287996934\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 904.794801\n",
      "---------------------------------------\n",
      "Total Timesteps: 166000 Episode Num: 166 Reward: 1262.5012548983639\n",
      "Total Timesteps: 167000 Episode Num: 167 Reward: -1319.7434172995222\n",
      "Total Timesteps: 168000 Episode Num: 168 Reward: -1744.5344812553865\n",
      "Total Timesteps: 169000 Episode Num: 169 Reward: -1661.876218673928\n",
      "Total Timesteps: 170000 Episode Num: 170 Reward: -1635.505490354016\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1661.923600\n",
      "---------------------------------------\n",
      "Total Timesteps: 171000 Episode Num: 171 Reward: -1624.8346744846413\n",
      "Total Timesteps: 172000 Episode Num: 172 Reward: -1672.1643725764548\n",
      "Total Timesteps: 173000 Episode Num: 173 Reward: -1898.8030816736361\n",
      "Total Timesteps: 174000 Episode Num: 174 Reward: -1745.1868914915283\n",
      "Total Timesteps: 175000 Episode Num: 175 Reward: -1113.6035656349961\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1374.717245\n",
      "---------------------------------------\n",
      "Total Timesteps: 176000 Episode Num: 176 Reward: -1390.920928072433\n",
      "Total Timesteps: 177000 Episode Num: 177 Reward: 671.1440071920387\n",
      "Total Timesteps: 178000 Episode Num: 178 Reward: 672.9003924411163\n",
      "Total Timesteps: 179000 Episode Num: 179 Reward: 634.7720982669938\n",
      "Total Timesteps: 180000 Episode Num: 180 Reward: 669.4789282512664\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 832.420989\n",
      "---------------------------------------\n",
      "Total Timesteps: 181000 Episode Num: 181 Reward: 716.0717320482803\n",
      "Total Timesteps: 182000 Episode Num: 182 Reward: 941.2904162504445\n",
      "Total Timesteps: 183000 Episode Num: 183 Reward: 1349.4836959109946\n",
      "Total Timesteps: 184000 Episode Num: 184 Reward: 1220.9400766118347\n",
      "Total Timesteps: 185000 Episode Num: 185 Reward: 1014.6832387968869\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 640.093106\n",
      "---------------------------------------\n",
      "Total Timesteps: 186000 Episode Num: 186 Reward: 593.7727703391513\n",
      "Total Timesteps: 187000 Episode Num: 187 Reward: 675.2418864945998\n",
      "Total Timesteps: 188000 Episode Num: 188 Reward: 804.3567103749488\n",
      "Total Timesteps: 189000 Episode Num: 189 Reward: 494.123195543457\n",
      "Total Timesteps: 190000 Episode Num: 190 Reward: 972.389833052305\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 772.275169\n",
      "---------------------------------------\n",
      "Total Timesteps: 191000 Episode Num: 191 Reward: 738.4882015055875\n",
      "Total Timesteps: 192000 Episode Num: 192 Reward: 768.7044661762683\n",
      "Total Timesteps: 193000 Episode Num: 193 Reward: 1018.2621238819629\n",
      "Total Timesteps: 194000 Episode Num: 194 Reward: 761.664842668176\n",
      "Total Timesteps: 195000 Episode Num: 195 Reward: 1304.0074968747012\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1393.748238\n",
      "---------------------------------------\n",
      "Total Timesteps: 196000 Episode Num: 196 Reward: 1555.4427845835542\n",
      "Total Timesteps: 197000 Episode Num: 197 Reward: 926.6346183355926\n",
      "Total Timesteps: 198000 Episode Num: 198 Reward: 1144.351308168504\n",
      "Total Timesteps: 199000 Episode Num: 199 Reward: 1010.7317809349709\n",
      "Total Timesteps: 200000 Episode Num: 200 Reward: 1285.3851970264059\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1451.560663\n",
      "---------------------------------------\n",
      "Total Timesteps: 201000 Episode Num: 201 Reward: 1459.0765531916352\n",
      "Total Timesteps: 202000 Episode Num: 202 Reward: 1205.461642096225\n",
      "Total Timesteps: 203000 Episode Num: 203 Reward: 1182.5986035821716\n",
      "Total Timesteps: 204000 Episode Num: 204 Reward: 929.0877302008231\n",
      "Total Timesteps: 205000 Episode Num: 205 Reward: 1256.1518493083163\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1027.426001\n",
      "---------------------------------------\n",
      "Total Timesteps: 206000 Episode Num: 206 Reward: 810.2223982989589\n",
      "Total Timesteps: 207000 Episode Num: 207 Reward: 818.3962373891613\n",
      "Total Timesteps: 208000 Episode Num: 208 Reward: 836.6799782772919\n",
      "Total Timesteps: 209000 Episode Num: 209 Reward: 798.9581455750425\n",
      "Total Timesteps: 210000 Episode Num: 210 Reward: 1223.4537623581602\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1199.869922\n",
      "---------------------------------------\n",
      "Total Timesteps: 211000 Episode Num: 211 Reward: 1225.4266872623493\n",
      "Total Timesteps: 212000 Episode Num: 212 Reward: 1336.7696790312154\n",
      "Total Timesteps: 213000 Episode Num: 213 Reward: 901.2795023080943\n",
      "Total Timesteps: 214000 Episode Num: 214 Reward: 1311.8032563493468\n",
      "Total Timesteps: 215000 Episode Num: 215 Reward: 1146.7615482587455\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1310.857776\n",
      "---------------------------------------\n",
      "Total Timesteps: 216000 Episode Num: 216 Reward: 1361.3592033557188\n",
      "Total Timesteps: 217000 Episode Num: 217 Reward: 1332.491112086807\n",
      "Total Timesteps: 218000 Episode Num: 218 Reward: 1350.9838806372682\n",
      "Total Timesteps: 219000 Episode Num: 219 Reward: 1178.0497301284406\n",
      "Total Timesteps: 220000 Episode Num: 220 Reward: 1386.4557563461321\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1760.543834\n",
      "---------------------------------------\n",
      "Total Timesteps: 221000 Episode Num: 221 Reward: 1642.5965621894504\n",
      "Total Timesteps: 222000 Episode Num: 222 Reward: 1711.743343452632\n",
      "Total Timesteps: 223000 Episode Num: 223 Reward: 1572.6202950901852\n",
      "Total Timesteps: 224000 Episode Num: 224 Reward: 1689.5793626811978\n",
      "Total Timesteps: 225000 Episode Num: 225 Reward: 1571.516845156899\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1745.028944\n",
      "---------------------------------------\n",
      "Total Timesteps: 226000 Episode Num: 226 Reward: 1664.7643925307605\n",
      "Total Timesteps: 227000 Episode Num: 227 Reward: 1683.5083000932611\n",
      "Total Timesteps: 228000 Episode Num: 228 Reward: 1291.579671908611\n",
      "Total Timesteps: 229000 Episode Num: 229 Reward: 1736.4978853881437\n",
      "Total Timesteps: 230000 Episode Num: 230 Reward: 1752.8116663954333\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1502.806251\n",
      "---------------------------------------\n",
      "Total Timesteps: 231000 Episode Num: 231 Reward: 1396.0280477031745\n",
      "Total Timesteps: 232000 Episode Num: 232 Reward: 1590.7870191407137\n",
      "Total Timesteps: 233000 Episode Num: 233 Reward: 1465.3110942041717\n",
      "Total Timesteps: 234000 Episode Num: 234 Reward: 1307.3437675789442\n",
      "Total Timesteps: 235000 Episode Num: 235 Reward: 1679.27099837348\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1577.142224\n",
      "---------------------------------------\n",
      "Total Timesteps: 236000 Episode Num: 236 Reward: 1583.8558446313862\n",
      "Total Timesteps: 237000 Episode Num: 237 Reward: 1409.3985183772318\n",
      "Total Timesteps: 238000 Episode Num: 238 Reward: 1814.2768472575183\n",
      "Total Timesteps: 239000 Episode Num: 239 Reward: 1588.9580733922708\n",
      "Total Timesteps: 240000 Episode Num: 240 Reward: 1848.668973563935\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1894.411669\n",
      "---------------------------------------\n",
      "Total Timesteps: 241000 Episode Num: 241 Reward: 1790.658403947414\n",
      "Total Timesteps: 242000 Episode Num: 242 Reward: 1689.8998663458874\n",
      "Total Timesteps: 243000 Episode Num: 243 Reward: 1616.418918859281\n",
      "Total Timesteps: 244000 Episode Num: 244 Reward: 1877.2742384510238\n",
      "Total Timesteps: 245000 Episode Num: 245 Reward: 1798.7308460544295\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1949.829580\n",
      "---------------------------------------\n",
      "Total Timesteps: 246000 Episode Num: 246 Reward: 1904.800822278587\n",
      "Total Timesteps: 247000 Episode Num: 247 Reward: 1877.4830541740985\n",
      "Total Timesteps: 248000 Episode Num: 248 Reward: 1961.7931600285046\n",
      "Total Timesteps: 249000 Episode Num: 249 Reward: 1252.6108372375325\n",
      "Total Timesteps: 250000 Episode Num: 250 Reward: 1440.2056924536585\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1597.172385\n",
      "---------------------------------------\n",
      "Total Timesteps: 251000 Episode Num: 251 Reward: 1557.4613895368145\n",
      "Total Timesteps: 252000 Episode Num: 252 Reward: 1746.508075390139\n",
      "Total Timesteps: 253000 Episode Num: 253 Reward: 1604.3450565370024\n",
      "Total Timesteps: 254000 Episode Num: 254 Reward: 1948.0472837543603\n",
      "Total Timesteps: 255000 Episode Num: 255 Reward: 1874.6625755601565\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2075.409478\n",
      "---------------------------------------\n",
      "Total Timesteps: 256000 Episode Num: 256 Reward: 2006.5043690182083\n",
      "Total Timesteps: 257000 Episode Num: 257 Reward: 1915.4247376724597\n",
      "Total Timesteps: 258000 Episode Num: 258 Reward: 2024.9928456940959\n",
      "Total Timesteps: 259000 Episode Num: 259 Reward: 1936.1227771128724\n",
      "Total Timesteps: 260000 Episode Num: 260 Reward: 2012.6738424680866\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2055.987072\n",
      "---------------------------------------\n",
      "Total Timesteps: 261000 Episode Num: 261 Reward: 2068.518945195533\n",
      "Total Timesteps: 262000 Episode Num: 262 Reward: 1951.0350441302865\n",
      "Total Timesteps: 263000 Episode Num: 263 Reward: 1770.8191320532937\n",
      "Total Timesteps: 264000 Episode Num: 264 Reward: 1965.3128119440182\n",
      "Total Timesteps: 265000 Episode Num: 265 Reward: 2024.0803811066555\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2064.985084\n",
      "---------------------------------------\n",
      "Total Timesteps: 266000 Episode Num: 266 Reward: 1959.3889387905904\n",
      "Total Timesteps: 267000 Episode Num: 267 Reward: 2017.3580557027296\n",
      "Total Timesteps: 268000 Episode Num: 268 Reward: 2067.3750512008346\n",
      "Total Timesteps: 269000 Episode Num: 269 Reward: 2065.1531675508245\n",
      "Total Timesteps: 270000 Episode Num: 270 Reward: 1907.7658811662786\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1862.303777\n",
      "---------------------------------------\n",
      "Total Timesteps: 271000 Episode Num: 271 Reward: 1871.0114950776065\n",
      "Total Timesteps: 272000 Episode Num: 272 Reward: 2036.8076461461108\n",
      "Total Timesteps: 273000 Episode Num: 273 Reward: 1869.3688980535649\n",
      "Total Timesteps: 274000 Episode Num: 274 Reward: 2039.2006427272463\n",
      "Total Timesteps: 275000 Episode Num: 275 Reward: 1930.2499505261783\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2077.767337\n",
      "---------------------------------------\n",
      "Total Timesteps: 276000 Episode Num: 276 Reward: 2072.826674942973\n",
      "Total Timesteps: 277000 Episode Num: 277 Reward: 1854.042865273326\n",
      "Total Timesteps: 278000 Episode Num: 278 Reward: 1939.4903251290073\n",
      "Total Timesteps: 279000 Episode Num: 279 Reward: 2004.9060525456136\n",
      "Total Timesteps: 280000 Episode Num: 280 Reward: 2034.9035243349094\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2273.270491\n",
      "---------------------------------------\n",
      "Total Timesteps: 281000 Episode Num: 281 Reward: 2233.419615100467\n",
      "Total Timesteps: 282000 Episode Num: 282 Reward: 2040.7701887655003\n",
      "Total Timesteps: 283000 Episode Num: 283 Reward: 2097.8866020647156\n",
      "Total Timesteps: 284000 Episode Num: 284 Reward: 1873.893542121861\n",
      "Total Timesteps: 285000 Episode Num: 285 Reward: 2170.1522524994093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2140.078062\n",
      "---------------------------------------\n",
      "Total Timesteps: 286000 Episode Num: 286 Reward: 2109.384592868355\n",
      "Total Timesteps: 287000 Episode Num: 287 Reward: 2159.559810610018\n",
      "Total Timesteps: 288000 Episode Num: 288 Reward: 2227.469775882039\n",
      "Total Timesteps: 289000 Episode Num: 289 Reward: 2058.1984570426775\n",
      "Total Timesteps: 290000 Episode Num: 290 Reward: 2160.6673868412045\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2191.323115\n",
      "---------------------------------------\n",
      "Total Timesteps: 291000 Episode Num: 291 Reward: 2159.845479366948\n",
      "Total Timesteps: 292000 Episode Num: 292 Reward: 2225.9053876733387\n",
      "Total Timesteps: 293000 Episode Num: 293 Reward: 2150.370882019162\n",
      "Total Timesteps: 294000 Episode Num: 294 Reward: 2037.7097353116426\n",
      "Total Timesteps: 295000 Episode Num: 295 Reward: 2133.1939632576737\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2235.644295\n",
      "---------------------------------------\n",
      "Total Timesteps: 296000 Episode Num: 296 Reward: 2198.272126265793\n",
      "Total Timesteps: 297000 Episode Num: 297 Reward: 2280.879801575932\n",
      "Total Timesteps: 298000 Episode Num: 298 Reward: 2282.9394668972336\n",
      "Total Timesteps: 299000 Episode Num: 299 Reward: 2156.0015192808746\n",
      "Total Timesteps: 300000 Episode Num: 300 Reward: 2259.258997126383\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2137.316885\n",
      "---------------------------------------\n",
      "Total Timesteps: 301000 Episode Num: 301 Reward: 2139.776540933475\n",
      "Total Timesteps: 302000 Episode Num: 302 Reward: 2240.9756303253007\n",
      "Total Timesteps: 303000 Episode Num: 303 Reward: 2259.4013298275718\n",
      "Total Timesteps: 304000 Episode Num: 304 Reward: 2312.780816682277\n",
      "Total Timesteps: 305000 Episode Num: 305 Reward: 2163.895909536608\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2239.523730\n",
      "---------------------------------------\n",
      "Total Timesteps: 306000 Episode Num: 306 Reward: 2204.267937943059\n",
      "Total Timesteps: 307000 Episode Num: 307 Reward: 2247.210226558073\n",
      "Total Timesteps: 308000 Episode Num: 308 Reward: 2201.646009151419\n",
      "Total Timesteps: 309000 Episode Num: 309 Reward: 2251.7734833092577\n",
      "Total Timesteps: 310000 Episode Num: 310 Reward: 2142.45844987841\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2100.660302\n",
      "---------------------------------------\n",
      "Total Timesteps: 311000 Episode Num: 311 Reward: 1965.0192879069223\n",
      "Total Timesteps: 312000 Episode Num: 312 Reward: 2246.8715218840493\n",
      "Total Timesteps: 313000 Episode Num: 313 Reward: 2244.759280300221\n",
      "Total Timesteps: 314000 Episode Num: 314 Reward: 2187.893018056057\n",
      "Total Timesteps: 315000 Episode Num: 315 Reward: 2212.960027725882\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2240.764901\n",
      "---------------------------------------\n",
      "Total Timesteps: 316000 Episode Num: 316 Reward: 2182.8606340583533\n",
      "Total Timesteps: 317000 Episode Num: 317 Reward: 2232.511326227124\n",
      "Total Timesteps: 318000 Episode Num: 318 Reward: 2258.3678208981146\n",
      "Total Timesteps: 319000 Episode Num: 319 Reward: 2280.409674164406\n",
      "Total Timesteps: 320000 Episode Num: 320 Reward: 1868.442096521624\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2304.443981\n",
      "---------------------------------------\n",
      "Total Timesteps: 321000 Episode Num: 321 Reward: 2168.394875042162\n",
      "Total Timesteps: 322000 Episode Num: 322 Reward: 2199.063588669735\n",
      "Total Timesteps: 323000 Episode Num: 323 Reward: 2225.294323130345\n",
      "Total Timesteps: 324000 Episode Num: 324 Reward: 2233.122355133795\n",
      "Total Timesteps: 325000 Episode Num: 325 Reward: 2320.9447376943776\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2255.694662\n",
      "---------------------------------------\n",
      "Total Timesteps: 326000 Episode Num: 326 Reward: 2248.7448939017813\n",
      "Total Timesteps: 327000 Episode Num: 327 Reward: 2194.0278434493125\n",
      "Total Timesteps: 328000 Episode Num: 328 Reward: 2212.5605273784395\n",
      "Total Timesteps: 329000 Episode Num: 329 Reward: 2158.7910203903575\n",
      "Total Timesteps: 330000 Episode Num: 330 Reward: 2344.884230829583\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2396.344182\n",
      "---------------------------------------\n",
      "Total Timesteps: 331000 Episode Num: 331 Reward: 2375.395925758872\n",
      "Total Timesteps: 332000 Episode Num: 332 Reward: 2273.1171939647666\n",
      "Total Timesteps: 333000 Episode Num: 333 Reward: 2323.2586330194927\n",
      "Total Timesteps: 334000 Episode Num: 334 Reward: 2192.2638676334886\n",
      "Total Timesteps: 335000 Episode Num: 335 Reward: 2250.4733415141486\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2293.328580\n",
      "---------------------------------------\n",
      "Total Timesteps: 336000 Episode Num: 336 Reward: 2261.251502526645\n",
      "Total Timesteps: 337000 Episode Num: 337 Reward: 2324.5856529221733\n",
      "Total Timesteps: 338000 Episode Num: 338 Reward: 2325.7006649275745\n",
      "Total Timesteps: 339000 Episode Num: 339 Reward: 2230.0828618478113\n",
      "Total Timesteps: 340000 Episode Num: 340 Reward: 2294.554957411797\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2333.003035\n",
      "---------------------------------------\n",
      "Total Timesteps: 341000 Episode Num: 341 Reward: 2319.0472477734206\n",
      "Total Timesteps: 342000 Episode Num: 342 Reward: 2145.3666586605577\n",
      "Total Timesteps: 343000 Episode Num: 343 Reward: 2302.602154987492\n",
      "Total Timesteps: 344000 Episode Num: 344 Reward: 2267.7239807523633\n",
      "Total Timesteps: 345000 Episode Num: 345 Reward: 2289.796458974611\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2286.174763\n",
      "---------------------------------------\n",
      "Total Timesteps: 346000 Episode Num: 346 Reward: 2234.865191129595\n",
      "Total Timesteps: 347000 Episode Num: 347 Reward: 2297.0453030762715\n",
      "Total Timesteps: 348000 Episode Num: 348 Reward: 2309.716251638349\n",
      "Total Timesteps: 349000 Episode Num: 349 Reward: 2362.202590429345\n",
      "Total Timesteps: 350000 Episode Num: 350 Reward: 1757.3390120806562\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2327.025314\n",
      "---------------------------------------\n",
      "Total Timesteps: 351000 Episode Num: 351 Reward: 2291.239355851017\n",
      "Total Timesteps: 352000 Episode Num: 352 Reward: 2211.808952411864\n",
      "Total Timesteps: 353000 Episode Num: 353 Reward: 2327.61192105546\n",
      "Total Timesteps: 354000 Episode Num: 354 Reward: 2341.5411298358504\n",
      "Total Timesteps: 355000 Episode Num: 355 Reward: 2306.411935031767\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2404.874667\n",
      "---------------------------------------\n",
      "Total Timesteps: 356000 Episode Num: 356 Reward: 2365.015340800677\n",
      "Total Timesteps: 357000 Episode Num: 357 Reward: 2428.773413488706\n",
      "Total Timesteps: 358000 Episode Num: 358 Reward: 2329.8679754436007\n",
      "Total Timesteps: 359000 Episode Num: 359 Reward: 2346.259514410199\n",
      "Total Timesteps: 360000 Episode Num: 360 Reward: 2380.0114867376237\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2363.613849\n",
      "---------------------------------------\n",
      "Total Timesteps: 361000 Episode Num: 361 Reward: 2329.920641190871\n",
      "Total Timesteps: 362000 Episode Num: 362 Reward: 2348.809582359049\n",
      "Total Timesteps: 363000 Episode Num: 363 Reward: 2376.181476711152\n",
      "Total Timesteps: 364000 Episode Num: 364 Reward: 2339.691861447387\n",
      "Total Timesteps: 365000 Episode Num: 365 Reward: 2358.173055864765\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2358.557068\n",
      "---------------------------------------\n",
      "Total Timesteps: 366000 Episode Num: 366 Reward: 2338.2759446231166\n",
      "Total Timesteps: 367000 Episode Num: 367 Reward: 2274.842429341579\n",
      "Total Timesteps: 368000 Episode Num: 368 Reward: 2372.7472575316897\n",
      "Total Timesteps: 369000 Episode Num: 369 Reward: 2359.183626043914\n",
      "Total Timesteps: 370000 Episode Num: 370 Reward: 2240.3954693900846\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2412.401263\n",
      "---------------------------------------\n",
      "Total Timesteps: 371000 Episode Num: 371 Reward: 2403.6134529565943\n",
      "Total Timesteps: 372000 Episode Num: 372 Reward: 2398.529629035926\n",
      "Total Timesteps: 373000 Episode Num: 373 Reward: 2406.2203983224495\n",
      "Total Timesteps: 374000 Episode Num: 374 Reward: 2359.1148137004366\n",
      "Total Timesteps: 375000 Episode Num: 375 Reward: 2332.220061745927\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2454.408887\n",
      "---------------------------------------\n",
      "Total Timesteps: 376000 Episode Num: 376 Reward: 2405.325405413431\n",
      "Total Timesteps: 377000 Episode Num: 377 Reward: 2409.388069456835\n",
      "Total Timesteps: 378000 Episode Num: 378 Reward: 2381.411066561354\n",
      "Total Timesteps: 379000 Episode Num: 379 Reward: 2490.7665163835527\n",
      "Total Timesteps: 380000 Episode Num: 380 Reward: 2412.5855713402466\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2309.536734\n",
      "---------------------------------------\n",
      "Total Timesteps: 381000 Episode Num: 381 Reward: 2304.679134737606\n",
      "Total Timesteps: 382000 Episode Num: 382 Reward: 2394.849564820511\n",
      "Total Timesteps: 383000 Episode Num: 383 Reward: 2334.77663548521\n",
      "Total Timesteps: 384000 Episode Num: 384 Reward: 2436.6141294913778\n",
      "Total Timesteps: 385000 Episode Num: 385 Reward: 2365.6372646335726\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2451.577816\n",
      "---------------------------------------\n",
      "Total Timesteps: 386000 Episode Num: 386 Reward: 2407.594862449182\n",
      "Total Timesteps: 387000 Episode Num: 387 Reward: 2476.5676785151377\n",
      "Total Timesteps: 388000 Episode Num: 388 Reward: 2460.335387240797\n",
      "Total Timesteps: 389000 Episode Num: 389 Reward: 2444.653493078707\n",
      "Total Timesteps: 390000 Episode Num: 390 Reward: 2404.839172483231\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2462.204465\n",
      "---------------------------------------\n",
      "Total Timesteps: 391000 Episode Num: 391 Reward: 2363.6452111409253\n",
      "Total Timesteps: 392000 Episode Num: 392 Reward: 2460.278380591646\n",
      "Total Timesteps: 393000 Episode Num: 393 Reward: 2453.9932616887936\n",
      "Total Timesteps: 394000 Episode Num: 394 Reward: 2504.2237685751747\n",
      "Total Timesteps: 395000 Episode Num: 395 Reward: 2320.257409967182\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2497.499777\n",
      "---------------------------------------\n",
      "Total Timesteps: 396000 Episode Num: 396 Reward: 2437.490009116572\n",
      "Total Timesteps: 397000 Episode Num: 397 Reward: 2482.564181681418\n",
      "Total Timesteps: 398000 Episode Num: 398 Reward: 2459.77796528282\n",
      "Total Timesteps: 399000 Episode Num: 399 Reward: 2391.9724060263748\n",
      "Total Timesteps: 400000 Episode Num: 400 Reward: 2276.0121244622405\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2504.071441\n",
      "---------------------------------------\n",
      "Total Timesteps: 401000 Episode Num: 401 Reward: 2439.8257324753636\n",
      "Total Timesteps: 402000 Episode Num: 402 Reward: 2238.592845837648\n",
      "Total Timesteps: 403000 Episode Num: 403 Reward: 2318.28372291701\n",
      "Total Timesteps: 404000 Episode Num: 404 Reward: 2448.681148694408\n",
      "Total Timesteps: 405000 Episode Num: 405 Reward: 2512.708350163827\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2342.584809\n",
      "---------------------------------------\n",
      "Total Timesteps: 406000 Episode Num: 406 Reward: 2294.3525442532678\n",
      "Total Timesteps: 407000 Episode Num: 407 Reward: 2417.4093060429373\n",
      "Total Timesteps: 408000 Episode Num: 408 Reward: 2387.8662931932954\n",
      "Total Timesteps: 409000 Episode Num: 409 Reward: 2444.375511561835\n",
      "Total Timesteps: 410000 Episode Num: 410 Reward: 2402.6897336807288\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2398.169909\n",
      "---------------------------------------\n",
      "Total Timesteps: 411000 Episode Num: 411 Reward: 2353.2606170993195\n",
      "Total Timesteps: 412000 Episode Num: 412 Reward: 1994.0932706879296\n",
      "Total Timesteps: 413000 Episode Num: 413 Reward: 2459.3420679821734\n",
      "Total Timesteps: 414000 Episode Num: 414 Reward: 2436.8138765264\n",
      "Total Timesteps: 415000 Episode Num: 415 Reward: 2408.974213535748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2434.181878\n",
      "---------------------------------------\n",
      "Total Timesteps: 416000 Episode Num: 416 Reward: 2368.2567533490796\n",
      "Total Timesteps: 417000 Episode Num: 417 Reward: 2326.94840240656\n",
      "Total Timesteps: 418000 Episode Num: 418 Reward: 2268.9289509255505\n",
      "Total Timesteps: 419000 Episode Num: 419 Reward: 2403.4677846111767\n",
      "Total Timesteps: 420000 Episode Num: 420 Reward: 2361.7086940475597\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2475.909527\n",
      "---------------------------------------\n",
      "Total Timesteps: 421000 Episode Num: 421 Reward: 2379.513765658487\n",
      "Total Timesteps: 422000 Episode Num: 422 Reward: 2258.1502677485214\n",
      "Total Timesteps: 423000 Episode Num: 423 Reward: 2417.8894663316796\n",
      "Total Timesteps: 424000 Episode Num: 424 Reward: 2430.169228951529\n",
      "Total Timesteps: 425000 Episode Num: 425 Reward: 2414.473588636033\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2511.040226\n",
      "---------------------------------------\n",
      "Total Timesteps: 426000 Episode Num: 426 Reward: 2481.278372895411\n",
      "Total Timesteps: 427000 Episode Num: 427 Reward: 2416.403722927775\n",
      "Total Timesteps: 428000 Episode Num: 428 Reward: 2530.915724129873\n",
      "Total Timesteps: 429000 Episode Num: 429 Reward: 2477.634005280775\n",
      "Total Timesteps: 430000 Episode Num: 430 Reward: 2445.8850217690947\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2566.656647\n",
      "---------------------------------------\n",
      "Total Timesteps: 431000 Episode Num: 431 Reward: 2516.668232459092\n",
      "Total Timesteps: 432000 Episode Num: 432 Reward: 2372.1929826898318\n",
      "Total Timesteps: 433000 Episode Num: 433 Reward: 2552.019298938717\n",
      "Total Timesteps: 434000 Episode Num: 434 Reward: 2419.579747050632\n",
      "Total Timesteps: 435000 Episode Num: 435 Reward: 2537.1014456516095\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2543.811240\n",
      "---------------------------------------\n",
      "Total Timesteps: 436000 Episode Num: 436 Reward: 2519.8400685610777\n",
      "Total Timesteps: 437000 Episode Num: 437 Reward: 2324.3003899744613\n",
      "Total Timesteps: 438000 Episode Num: 438 Reward: 2369.3041681034347\n",
      "Total Timesteps: 439000 Episode Num: 439 Reward: 2401.3692568627616\n",
      "Total Timesteps: 440000 Episode Num: 440 Reward: 2531.6154148269584\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2573.819784\n",
      "---------------------------------------\n",
      "Total Timesteps: 441000 Episode Num: 441 Reward: 2554.997684174659\n",
      "Total Timesteps: 442000 Episode Num: 442 Reward: 2506.453791743691\n",
      "Total Timesteps: 443000 Episode Num: 443 Reward: 2515.406727027868\n",
      "Total Timesteps: 444000 Episode Num: 444 Reward: 2379.68033902135\n",
      "Total Timesteps: 445000 Episode Num: 445 Reward: 2536.7394967636183\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2577.188863\n",
      "---------------------------------------\n",
      "Total Timesteps: 446000 Episode Num: 446 Reward: 2558.7816338772936\n",
      "Total Timesteps: 447000 Episode Num: 447 Reward: 2514.600590370468\n",
      "Total Timesteps: 448000 Episode Num: 448 Reward: 2500.7146647399745\n",
      "Total Timesteps: 449000 Episode Num: 449 Reward: 2493.287937584807\n",
      "Total Timesteps: 450000 Episode Num: 450 Reward: 2511.6398754686006\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2556.622643\n",
      "---------------------------------------\n",
      "Total Timesteps: 451000 Episode Num: 451 Reward: 2536.5827525500194\n",
      "Total Timesteps: 452000 Episode Num: 452 Reward: 2569.940666273081\n",
      "Total Timesteps: 453000 Episode Num: 453 Reward: 2460.5450965548184\n",
      "Total Timesteps: 454000 Episode Num: 454 Reward: 2458.869224067582\n",
      "Total Timesteps: 455000 Episode Num: 455 Reward: 2510.2474868629474\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2613.216588\n",
      "---------------------------------------\n",
      "Total Timesteps: 456000 Episode Num: 456 Reward: 2555.605803316222\n",
      "Total Timesteps: 457000 Episode Num: 457 Reward: 2595.9696377295695\n",
      "Total Timesteps: 458000 Episode Num: 458 Reward: 2443.641703745486\n",
      "Total Timesteps: 459000 Episode Num: 459 Reward: 2557.288939835227\n",
      "Total Timesteps: 460000 Episode Num: 460 Reward: 2565.5285832197037\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2567.022928\n",
      "---------------------------------------\n",
      "Total Timesteps: 461000 Episode Num: 461 Reward: 2536.194111052384\n",
      "Total Timesteps: 462000 Episode Num: 462 Reward: 2591.8026941106828\n",
      "Total Timesteps: 463000 Episode Num: 463 Reward: 2598.8048705199762\n",
      "Total Timesteps: 464000 Episode Num: 464 Reward: 2565.50985142789\n",
      "Total Timesteps: 465000 Episode Num: 465 Reward: 2611.9787627092355\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2573.804541\n",
      "---------------------------------------\n",
      "Total Timesteps: 466000 Episode Num: 466 Reward: 2522.5157380779083\n",
      "Total Timesteps: 467000 Episode Num: 467 Reward: 2562.042204032949\n",
      "Total Timesteps: 468000 Episode Num: 468 Reward: 2577.8788767534243\n",
      "Total Timesteps: 469000 Episode Num: 469 Reward: 2561.404135221178\n",
      "Total Timesteps: 470000 Episode Num: 470 Reward: 2575.021283671758\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2646.831870\n",
      "---------------------------------------\n",
      "Total Timesteps: 471000 Episode Num: 471 Reward: 2604.387146469107\n",
      "Total Timesteps: 472000 Episode Num: 472 Reward: 2577.618119788305\n",
      "Total Timesteps: 473000 Episode Num: 473 Reward: 2645.1321547758926\n",
      "Total Timesteps: 474000 Episode Num: 474 Reward: 2628.162505572769\n",
      "Total Timesteps: 475000 Episode Num: 475 Reward: 2574.9277350438933\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2636.129473\n",
      "---------------------------------------\n",
      "Total Timesteps: 476000 Episode Num: 476 Reward: 2633.71411165348\n",
      "Total Timesteps: 477000 Episode Num: 477 Reward: 2502.3738498164394\n",
      "Total Timesteps: 478000 Episode Num: 478 Reward: 2590.23244651326\n",
      "Total Timesteps: 479000 Episode Num: 479 Reward: 2570.0067933019614\n",
      "Total Timesteps: 480000 Episode Num: 480 Reward: 2551.957100532328\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2620.382708\n",
      "---------------------------------------\n",
      "Total Timesteps: 481000 Episode Num: 481 Reward: 2571.3382152867594\n",
      "Total Timesteps: 482000 Episode Num: 482 Reward: 2463.9994517490436\n",
      "Total Timesteps: 483000 Episode Num: 483 Reward: 2648.99242372213\n",
      "Total Timesteps: 484000 Episode Num: 484 Reward: 2528.5051008601754\n",
      "Total Timesteps: 485000 Episode Num: 485 Reward: 2532.828518612273\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2607.235357\n",
      "---------------------------------------\n",
      "Total Timesteps: 486000 Episode Num: 486 Reward: 2542.5492918638124\n",
      "Total Timesteps: 487000 Episode Num: 487 Reward: 2493.356898222851\n",
      "Total Timesteps: 488000 Episode Num: 488 Reward: 2647.390201332247\n",
      "Total Timesteps: 489000 Episode Num: 489 Reward: 2626.992373756351\n",
      "Total Timesteps: 490000 Episode Num: 490 Reward: 2432.3382476244474\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2628.149055\n",
      "---------------------------------------\n",
      "Total Timesteps: 491000 Episode Num: 491 Reward: 2570.7708154264687\n",
      "Total Timesteps: 492000 Episode Num: 492 Reward: 2397.971054752616\n",
      "Total Timesteps: 493000 Episode Num: 493 Reward: 2294.251979723516\n",
      "Total Timesteps: 494000 Episode Num: 494 Reward: 2577.5164010675285\n",
      "Total Timesteps: 495000 Episode Num: 495 Reward: 2412.3028855262323\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2604.912799\n",
      "---------------------------------------\n",
      "Total Timesteps: 496000 Episode Num: 496 Reward: 2564.5364885039407\n",
      "Total Timesteps: 497000 Episode Num: 497 Reward: 2657.4808668559685\n",
      "Total Timesteps: 498000 Episode Num: 498 Reward: 2589.3522002585614\n",
      "Total Timesteps: 499000 Episode Num: 499 Reward: 2672.386188115148\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2700.706914\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Nous démarrons la boucle principale en 500 000 pas\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # Si l'épisode est terminé\n",
    "  if done:\n",
    "\n",
    "    # Si nous ne sommes pas au tout début, nous commençons le processus de formation du modèle\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    #  Nous évaluons l'épisode et nous sauvegardons la politique\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # Lorsque l'étape de l'entrainement est terminée, nous réinitialisons l'état de l'environnement\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Mettre Done à False (Si l'épisode n'est pas terminé)\n",
    "    done = False\n",
    "    \n",
    "    # Mettre à zéro les récompenses et les pas de temps des épisodes\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Avant 10000 pas de temps, nous jouons à des actions aléatoires\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # Après 10 000 pas, nous changeons de modèle\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # Si le paramètre explore_noise n'est pas 0, nous ajoutons du bruit à l'action et nous la découpons\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # L'agent effectue l'action dans l'environnement, puis passe à l'état suivant et reçoit la récompense\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # Nous vérifions si l'épisode est terminé\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # Nous augmentons la récompense totale\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # Nous enregistrons la nouvelle transition dans Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # Nous mettons à jour l'état, la durée de l'épisode, la durée totale et la durée depuis l'évaluation de la politique\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# Nous ajoutons la dernière évaluation de la politique à notre liste d'évaluations et nous sauvegardons notre modèle\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 107403,
     "status": "ok",
     "timestamp": 1601860665473,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "oW4d1YAMqif1",
    "outputId": "ad2af8a8-3934-4e5a-b467-8cd101c2439c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2698.533188\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Définir le premier réseau neuronal critique\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Définir le second réseau neuronal critique\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Propagation de l'information sur le premier réseau neural critique\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Propagation de l'information sur le deuxième réseau neuronal critique\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Device selection (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Intégrer l'ensemble du processus de formation dans une classe\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Étape 4: Nous échantillonnons un lot de transitions (s, s’, a, r) de la mémoire\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Étape 5: De l'état suivant s', l'acteur cible joue l'action suivante a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Étape 6: Nous ajoutons le bruit gaussien à cette prochaine action a’ et nous l'inscrivons dans une série de valeurs soutenues par l'environnement\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Étape 7: Les deux cibles des critiques prennent chacune le couple (s’, a’) comme input et retourne deux Q-values Qt1(s’,a’) et Qt2(s’,a’) comme outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Étape 8: Nous gardons le minimum de ces deux Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Étape 9: Nous obtenons la cible finale des deux modèles critiques, qui est: Qt = r + γ * min(Qt1, Qt2), où γ est le coefficient d'actualisation\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Étape 10: Les deux modèles critiques prennent chacun le couple (s, a) comme input et retourne deux Q-values Q1(s,a) et Q2(s,a) comme outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Étape 11: Nous calculons la perte à partir des deux modèles critiques: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Étape 12: Nous rétropropageons cette perte Critic et mettons à jour les paramètres des deux modèles Critic avec un optimiseur SGD\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Étape 13:  Une fois pour chaque deux itérations, nous mettons à jour notre modèle Acteur en effectuant une montée du gradient sur la sortie du premier modèle Critique\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Étape 14: Une fois pour chaque deux itérations, nous mettons à jour les poids de l'acteur cible en calculant la moyenne des polyak\n",
    "#  Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Étape 15 : Une fois pour chaque deux itérations, nous mettons à jour les poids de la cible critique en calculant la moyenne des polyak\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Faire une méthode de sauvegarde pour sauvegarder un modèle entraîné\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Mise au point d'une méthode de chargement pour charger un modèle pré-entrainé\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1601860102189,
     "user": {
      "displayName": "Dicken MOUNGALA",
      "photoUrl": "",
      "userId": "10428232438529067233"
     },
     "user_tz": -420
    },
    "id": "t1pKsWbvNcWl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Half_Cheetah.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
